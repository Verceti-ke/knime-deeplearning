<?xml version="1.0" encoding="UTF-8"?>
<knimeNode icon="dlkerastrelulayer.png" type="Other" xmlns="http://knime.org/node/v3.6" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://knime.org/node/v3.6 http://knime.org/node/v3.6.xsd">
	<name>Keras Thresholded ReLU Layer</name>

	<shortDescription>
		A thresholded rectified linear unit layer.
	</shortDescription>

	<fullDescription>
		<intro>
			Similar to ordinary ReLUs but shifted by theta.
			<tt>f(x) = x for x &gt; theta, f(x) = 0 otherwise</tt>.
			Corresponds to the
			<a href="https://keras.io/layers/advanced-activations/#thresholdedrelu">Keras Thresholded ReLU Layer</a>.
		</intro>
		<option name="Name prefix">
			The name prefix of the layer. The prefix is complemented by an index suffix to obtain a unique layer name. If this option is unchecked, the name prefix is derived from the layer type.
		</option>
		<option name="Theta">
			Threshold location of activation.
			A theta of 0 corresponds to an ordinary ReLU.
		</option>
		<link href="https://www.knime.com/deeplearning/keras">
			KNIME Deep Learning Keras Integration
		</link>
	</fullDescription>
	<ports>
		<inPort index="0" name="Keras Network">
			The Keras deep learning network to which to add a
			<tt>Thresholded ReLU</tt>
			layer.
		</inPort>
		<outPort index="0" name="Keras Network">
			The Keras deep learning network with an added
			<tt>Thresholded ReLU</tt>
			layer.
		</outPort>
	</ports>
</knimeNode>
